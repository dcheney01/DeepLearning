{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcheney01/DeepLearning/blob/main/Sequence_to_Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "### Description:\n",
        "This is my version of [Karpathy's char-rnn model](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "I implemented my own version of Pytorch's GRU and trained this model on [SciFi Stories Text Corpus.](https://www.kaggle.com/jannesklaas/scifi-stories-text-corpus) The model ended up learning punctuation pretty well, but the spelling needs some work.\n",
        "\n",
        "I used [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) and [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) as resources for this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bdZWxvJrsx",
        "outputId": "aa36ec0c-5187-42c6-8ecd-3519a59324ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): text_files.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.3-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 12.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0_WitWJ99e"
      },
      "outputs": [],
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3p2Xl6Zshs2"
      },
      "outputs": [],
      "source": [
        "# grab random chunk from text dataset\n",
        "def random_chunk(file, chunk_len):\n",
        "  start_index = random.randint(0, len(file) - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "def random_training_set(file, chunk_len):    \n",
        "  chunk = random_chunk(file, chunk_len)\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # nn.module list and add a new linear layer for each layer\n",
        "    self.ir = nn.ModuleList([nn.Linear(input_size, hidden_size) for i in range(num_layers)])\n",
        "    self.hr = nn.ModuleList([nn.Linear(input_size, hidden_size) for i in range(num_layers)])\n",
        "\n",
        "    self.iz = nn.ModuleList([nn.Linear(input_size, hidden_size) for i in range(num_layers)])\n",
        "    self.hz = nn.ModuleList([nn.Linear(input_size, hidden_size) for i in range(num_layers)])\n",
        "\n",
        "    self.i_n = nn.ModuleList([nn.Linear(input_size, hidden_size) for i in range(num_layers)])\n",
        "    self.hn = nn.ModuleList([nn.Linear(input_size, hidden_size) for i in range(num_layers)])\n",
        "\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    # output of forward is the next hidden layer\n",
        "    xt = inputs\n",
        "    hidden_list = []\n",
        "\n",
        "    for layer in range(self.num_layers):\n",
        "      ht = hidden[layer]\n",
        "      ir = self.ir[layer]\n",
        "      hr = self.hr[layer]\n",
        "      iz = self.iz[layer]\n",
        "      hz = self.hz[layer]\n",
        "      i_n = self.i_n[layer]\n",
        "      hn = self.hn[layer]\n",
        "\n",
        "      reset_gate = torch.sigmoid(ir(xt) + hr(ht))\n",
        "      update_gate = torch.sigmoid(iz(xt) + hz(ht))\n",
        "      n_t = torch.tanh(i_n(xt) + (reset_gate * hn(ht)))\n",
        "      ht_new = (1 - update_gate) * n_t + (update_gate * ht)\n",
        "\n",
        "      # input to the next layer is \n",
        "      xt = ht_new\n",
        "      # store ht_new for each layer\n",
        "      hidden_list.append(ht_new.unsqueeze(0))\n",
        "      \n",
        "    return ht_new, torch.cat(hidden_list, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = GRU(input_size, hidden_size, n_layers)\n",
        "    # Translates from a hidden layer to actual output\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    out_decoded = F.relu(self.embedding(input_char).view(1, 1, -1))\n",
        "    out_decoded, hidden = self.gru(out_decoded, hidden)\n",
        "    out_decoded = self.out(out_decoded[0])\n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "def train(decoder, decoder_optimizer, inp, target):\n",
        "  decoder_optimizer.zero_grad()\n",
        "  loss = 0\n",
        "\n",
        "  input_length = inp.size(0)\n",
        "  target_length = target.size(0)\n",
        "\n",
        "  decoder_hidden = decoder.init_hidden()\n",
        "  decoder_input = inp\n",
        "  \n",
        "  for di in range(target_length):\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input[di], decoder_hidden)\n",
        "    loss += criterion(decoder_output, target[di].unsqueeze(dim=0))\n",
        "  \n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "  with torch.no_grad():\n",
        "\n",
        "    decoder_hidden = decoder.init_hidden()\n",
        "\n",
        "    decoded_words = []\n",
        "    \n",
        "    decoded_words.append(prime_str)\n",
        "\n",
        "    for char in prime_str:\n",
        "      decoder_output, decoder_hidden = decoder(char_tensor(char), decoder_hidden)\n",
        "\n",
        "    decoder_input = char_tensor(prime_str[-1])\n",
        "\n",
        "    for di in range(predict_len):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      decoder_input = sample_outputs(decoder_output.squeeze(dim=0).squeeze(dim=0), temperature)\n",
        "      decoded_words.append(all_characters[decoder_input.item()])\n",
        "\n",
        "  return ''.join(decoded_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKfozqw-6eqb"
      },
      "outputs": [],
      "source": [
        "def train_model(decoder, decoder_optimizer, n_epochs, predict_len, file, chunk_len, print_every):\n",
        "  epoch_progress = tqdm(total=n_epochs, position=0, leave=False)\n",
        "  start = time.time()\n",
        "\n",
        "  all_losses = []\n",
        "  loss_avg = 0\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "          # train(decoder, decoder_optimizer, inp, target):\n",
        "    loss_ = train(decoder, decoder_optimizer, *random_training_set(file, chunk_len))       \n",
        "    loss_avg += loss_\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        print(evaluate(decoder, 'Wh', predict_len), '\\n')\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        all_losses.append(loss_avg / 10)\n",
        "        loss_avg = 0\n",
        "    \n",
        "    epoch_progress.set_description('epoch:{} loss:{:.4f} '.format(epoch, loss_))\n",
        "    epoch_progress.update(1)\n",
        "\n",
        "  print(\"Done Training, output below:\")\n",
        "\n",
        "def output_model(decoder, predict_len):\n",
        "    for i in range(10):\n",
        "      start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \"r\", \"T\", \"J\", \"B\", \"lo\", \"we\", \"po\"]\n",
        "      start = random.randint(0,len(start_strings)-1)\n",
        "      print(start_strings[start])\n",
        "      print(evaluate(decoder, start_strings[start], predict_len), '\\n')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659,
          "referenced_widgets": [
            "c7fdabbbe06949af9d8d536bdd6c94cd",
            "4a1eab7ab90f4a24a62503ee109ce30c",
            "e24f8337702b4d929a3579129664b2e6",
            "8eb62bd1b1b946aab28f616d56c41fab",
            "7a58737de6fc476c989c621f017f20cb",
            "eb0a5a5a29194ed4964a7c7817910d38",
            "0f34b99a21324e7f964b095eda8fc8bc",
            "289a935132ec4250821f3ad4244e581f",
            "19b1ed1ed9074476ac734e6187ad81c3",
            "6396d838d03b4fa69115e0a0d0e27ebc",
            "cc5fe75512054213b103910a1c9b7d82"
          ]
        },
        "id": "HvPMid2hUOL5",
        "outputId": "8a92d1ba-90b0-437b-d768-0cc7b8a5bcb1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7fdabbbe06949af9d8d536bdd6c94cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5316.354083299637 (10000 100%) 331.7824]\n",
            "Whus and so the rate thace. About the direction one alter prout are gam. # Nical Russivise of Enjumped \n",
            "\n",
            "Done Training, output below:\n",
            "r\n",
            "rular or slaunded on his cautiously. \"We don't yes -- there peed.\" \"What have ruck back doing.\" \"Let's seet of the beds to be bread things peridable. # Khy down and then wowen in the picked astonding a \n",
            "\n",
            "T\n",
            "Tips of his desk to the selled about so which was all and on the great had endless to cold the liveral agail chad loares aptain to my change should do seour that she top stars that becail, good man bee \n",
            "\n",
            "we\n",
            "wen back, he could exchand, someone this this some your told him on the stared you gill bit for through the seeting to even only the risons one before I heard strend many your flament your and study fou \n",
            "\n",
            " he\n",
            " hel study plass of the must corrary manuse also down and reading his fast. He said the colds were in the hand by helping peristical soul time of the very love his barracing out the man ready levers stop \n",
            "\n",
            "T\n",
            "That was fandy -- \" \"He absome the metallign. W. Good of the correming to be an goes I couldn't have got come. \"The In't you go the fear, it said. One.\" \"And he said on on the grinning another good aga \n",
            "\n",
            " Th\n",
            " Thip what room come in a childrens of the man here, battle and before the should conopel was walknably trees. \"Bull or mess high it in a giller -- \" \"Dance.\" And we tended on the other sight and what he \n",
            "\n",
            "r\n",
            "race conver of it a small to stark the slip, need by the Avery put not became to be since where I didn't know a face whether down it then Kelt. It come sympered. The conscures to the possired with his  \n",
            "\n",
            "lo\n",
            "lok to you did for the carest.\" \"Paga,\" said dam glanded # Based Kraster IF w no throw what then he kneit of a leed that the people so and this great to the three to look blood grawls the fidion what. T \n",
            "\n",
            "po\n",
            "pot anally fit it's way turned my meases of the sense.\" \"You're Amoarm..\" \"See here to sure with my smokes, the water away, in an own longer that truen at the great, you stared it was there was hands it \n",
            "\n",
            "po\n",
            "pon for each through with it,\" she knowd three hened. \"A hundary that rest a arm staring for see parts at the elemenoze of an explot.\" \"Go the well doing the dia sir.\" she was a way moving the spirculat \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run for SciFi dataset\n",
        "scifi_file = unidecode.unidecode(open('/content/internet_archive_scifi_v3.txt').read())\n",
        "\n",
        "#               RNN(input_size,   hidden_size, output_size,  n_layers=1):\n",
        "scifi_decoder = RNN(n_characters, 100,         n_characters, 3)\n",
        "\n",
        "scifi_optimizer = torch.optim.Adam(scifi_decoder.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "# train_model(decoder,       decoder_optimizer,  n_epochs, predict_len, file,       chunk_len, print_every)\n",
        "train_model(  scifi_decoder, scifi_optimizer,    10000,     100,         scifi_file, 200,       10000)\n",
        "\n",
        "# output_model(decoder,       predict_len):\n",
        "output_model(  scifi_decoder, predict_len=200)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7fdabbbe06949af9d8d536bdd6c94cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4a1eab7ab90f4a24a62503ee109ce30c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e24f8337702b4d929a3579129664b2e6",
              "IPY_MODEL_8eb62bd1b1b946aab28f616d56c41fab",
              "IPY_MODEL_7a58737de6fc476c989c621f017f20cb"
            ]
          }
        },
        "4a1eab7ab90f4a24a62503ee109ce30c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e24f8337702b4d929a3579129664b2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb0a5a5a29194ed4964a7c7817910d38",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "epoch:10000 loss:331.7824 : 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f34b99a21324e7f964b095eda8fc8bc"
          }
        },
        "8eb62bd1b1b946aab28f616d56c41fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_289a935132ec4250821f3ad4244e581f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19b1ed1ed9074476ac734e6187ad81c3"
          }
        },
        "7a58737de6fc476c989c621f017f20cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6396d838d03b4fa69115e0a0d0e27ebc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [1:28:36&lt;00:00,  1.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc5fe75512054213b103910a1c9b7d82"
          }
        },
        "eb0a5a5a29194ed4964a7c7817910d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f34b99a21324e7f964b095eda8fc8bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "289a935132ec4250821f3ad4244e581f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19b1ed1ed9074476ac734e6187ad81c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6396d838d03b4fa69115e0a0d0e27ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc5fe75512054213b103910a1c9b7d82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}